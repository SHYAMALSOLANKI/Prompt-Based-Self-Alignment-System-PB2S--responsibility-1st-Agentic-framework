#!/usr/bin/env python3
"""
KoboldCpp + Brain Integration Setup
Optimized for your specific GGUF models and hardware
"""
import subprocess
import sys
import os
import requests
import time
import json
from pathlib import Path
import psutil

class KoboldBrainSetup:
    def __init__(self):
        self.script_dir = Path(__file__).parent
        self.kobold_exe = self.script_dir / "koboldcpp_rocm_b2.exe"
        self.models_dir = self.script_dir
        
        # Your available models
        self.available_models = {
            "gemma-3-4b": {
                "file": "gemma-3-4b-it-Q4_K_M.gguf",
                "size_gb": 2.4,
                "ram_needed": 4,
                "description": "Google Gemma 3 4B - Excellent for conversation and reasoning",
                "recommended": True
            },
            "qwen2.5-vl-7b": {
                "file": "qwen2.5-vl-7b-vision-mmproj-f16.gguf", 
                "size_gb": 1.3,
                "ram_needed": 3,
                "description": "Qwen 2.5 VL 7B - Advanced vision + language model",
                "recommended": True
            },
            "qwen2-vl-2b": {
                "file": "Qwen2-VL-2B-mmproj-q5_1.gguf",
                "size_gb": 0.5,
                "ram_needed": 2,
                "description": "Qwen 2 VL 2B - Lightweight vision model",
                "recommended": False
            },
            "llama-13b-proj": {
                "file": "llama-13b-mmproj-v1.5.Q4_1.gguf",
                "size_gb": 0.2,
                "ram_needed": 1,
                "description": "LLaMA 13B projection layer",
                "recommended": False
            },
            "llama3-8b-proj": {
                "file": "LLaMA3-8B_mmproj-Q4_1.gguf",
                "size_gb": 0.2,
                "ram_needed": 1,
                "description": "LLaMA 3 8B projection layer", 
                "recommended": False
            }
        }
    
    def check_system_specs(self):
        """Analyze current system capabilities"""
        print("ğŸ” SYSTEM ANALYSIS")
        print("=" * 50)
        
        # RAM
        ram_gb = psutil.virtual_memory().total / (1024**3)
        print(f"ğŸ’¾ RAM: {ram_gb:.1f} GB")
        
        # CPU
        cpu_count = psutil.cpu_count()
        cpu_freq = psutil.cpu_freq()
        print(f"ğŸ§  CPU: {cpu_count} cores @ {cpu_freq.max:.0f} MHz")
        
        # Storage
        disk = psutil.disk_usage(str(self.script_dir))
        free_gb = disk.free / (1024**3)
        print(f"ğŸ’¿ Free Storage: {free_gb:.1f} GB")
        
        # Recommendations
        print("\nğŸ¯ MODEL RECOMMENDATIONS:")
        print("-" * 30)
        
        for model_id, info in self.available_models.items():
            model_file = self.models_dir / info["file"]
            exists = "âœ…" if model_file.exists() else "âŒ"
            
            if ram_gb >= info["ram_needed"]:
                ram_status = "âœ…"
            else:
                ram_status = "âš ï¸"
            
            if info["recommended"]:
                star = "â­"
            else:
                star = "  "
            
            print(f"{star} {exists} {ram_status} {model_id}")
            print(f"     ğŸ“ {info['file']}")
            print(f"     ğŸ’¾ Needs {info['ram_needed']}GB RAM")
            print(f"     ğŸ“ {info['description']}")
            print()
        
        return {
            'ram_gb': ram_gb,
            'cpu_count': cpu_count,
            'free_storage_gb': free_gb,
            'can_run_gemma': ram_gb >= 4,
            'can_run_qwen_7b': ram_gb >= 3,
            'can_run_qwen_2b': ram_gb >= 2
        }
    
    def recommend_best_model(self, system_specs):
        """Recommend the best model for current system"""
        ram_gb = system_specs['ram_gb']
        
        if ram_gb >= 4:
            return "gemma-3-4b", "ğŸŒŸ Excellent choice! Gemma 3 4B will run smoothly"
        elif ram_gb >= 3:
            return "qwen2.5-vl-7b", "ğŸ¯ Good choice! Qwen 2.5 VL 7B with vision capabilities"
        elif ram_gb >= 2:
            return "qwen2-vl-2b", "âš¡ Lightweight option: Qwen 2 VL 2B"
        else:
            return None, "âŒ Need at least 2GB RAM for AI models"
    
    def start_koboldcpp_server(self, model_choice, custom_params=None):
        """Start KoboldCpp server with optimal settings"""
        model_info = self.available_models[model_choice]
        model_path = self.models_dir / model_info["file"]
        
        if not model_path.exists():
            print(f"âŒ Model file not found: {model_path}")
            return None
        
        print(f"ğŸš€ Starting KoboldCpp with {model_choice}...")
        print(f"ğŸ“ Model: {model_path}")
        
        # Base command
        cmd = [
            str(self.kobold_exe),
            str(model_path),
            "--host", "127.0.0.1",
            "--port", "5001",  # Different from default to avoid conflicts
            "--contextsize", "2048",
            "--threads", str(min(psutil.cpu_count(), 8)),
        ]
        
        # Add GPU acceleration if available (ROCm for AMD)
        if "rocm" in str(self.kobold_exe).lower():
            cmd.extend(["--gpulayers", "99"])  # Use all GPU layers
            print("ğŸ”¥ Using AMD ROCm GPU acceleration")
        
        # Custom parameters
        if custom_params:
            cmd.extend(custom_params)
        
        print(f"âš¡ Command: {' '.join(cmd)}")
        print("ğŸŒ Server will be available at: http://localhost:5001")
        print("â³ Starting server (this may take a minute)...")
        
        try:
            # Start KoboldCpp
            process = subprocess.Popen(
                cmd,
                cwd=str(self.script_dir),
                stdout=subprocess.PIPE,
                stderr=subprocess.PIPE,
                creationflags=subprocess.CREATE_NEW_CONSOLE if sys.platform == "win32" else 0
            )
            
            # Wait for server to start
            print("â³ Waiting for server to initialize...")
            for i in range(30):  # Wait up to 30 seconds
                try:
                    response = requests.get("http://localhost:5001/api/v1/model", timeout=2)
                    if response.status_code == 200:
                        print("âœ… KoboldCpp server is running!")
                        print(f"ğŸ§  Model loaded: {response.json().get('result', 'Unknown')}")
                        return process
                except:
                    pass
                
                time.sleep(1)
                print(f"â³ Still starting... ({i+1}/30)")
            
            print("âš ï¸ Server may still be starting. Check the KoboldCpp window.")
            return process
            
        except Exception as e:
            print(f"âŒ Failed to start KoboldCpp: {e}")
            return None
    
    def test_kobold_connection(self):
        """Test connection to KoboldCpp server"""
        print("ğŸ§ª Testing KoboldCpp connection...")
        
        try:
            # Test model endpoint
            response = requests.get("http://localhost:5001/api/v1/model", timeout=5)
            if response.status_code == 200:
                model_info = response.json()
                print("âœ… KoboldCpp is responding!")
                print(f"ğŸ“‹ Model: {model_info.get('result', 'Unknown')}")
                
                # Test generation
                test_prompt = {
                    "prompt": "Hello! How are you today?",
                    "max_length": 50,
                    "temperature": 0.7
                }
                
                gen_response = requests.post(
                    "http://localhost:5001/api/v1/generate", 
                    json=test_prompt,
                    timeout=30
                )
                
                if gen_response.status_code == 200:
                    result = gen_response.json()
                    print("âœ… Text generation working!")
                    print(f"ğŸ¤– Response: {result.get('results', [{}])[0].get('text', 'No response')[:100]}...")
                    return True
                else:
                    print(f"âš ï¸ Generation failed: {gen_response.status_code}")
                    return False
            else:
                print(f"âŒ Server not responding: {response.status_code}")
                return False
                
        except Exception as e:
            print(f"âŒ Connection failed: {e}")
            return False
    
    def configure_brain_for_kobold(self):
        """Configure brain to use KoboldCpp server"""
        config = {
            "llm_endpoints": [
                {
                    "name": "KoboldCpp",
                    "url": "http://localhost:5001/api/v1",
                    "type": "kobold_compatible",
                    "priority": 1,
                    "enabled": True
                },
                {
                    "name": "LM Studio",
                    "url": "http://localhost:1234/v1",
                    "type": "openai_compatible",
                    "priority": 2,
                    "enabled": True
                }
            ],
            "default_model": "auto",
            "timeout": 60,
            "retry_attempts": 3,
            "generation_params": {
                "max_length": 512,
                "temperature": 0.7,
                "top_p": 0.9,
                "rep_pen": 1.1
            }
        }
        
        config_path = self.script_dir / "brain_llm_config.json"
        with open(config_path, 'w') as f:
            json.dump(config, f, indent=2)
        
        print(f"âœ… Brain configured for KoboldCpp: {config_path}")
        return config_path
    
    def assess_hardware_needs(self, system_specs):
        """Assess if additional hardware is needed"""
        print("\nğŸ—ï¸ HARDWARE ASSESSMENT")
        print("=" * 50)
        
        ram_gb = system_specs['ram_gb']
        
        if ram_gb >= 8:
            print("ğŸ‰ EXCELLENT SETUP!")
            print("âœ… Your laptop can run large models smoothly")
            print("âœ… Can handle Gemma 3 4B + system overhead")
            print("âœ… No additional hardware needed")
            recommendation = "continue_current"
        elif ram_gb >= 4:
            print("ğŸ¯ GOOD SETUP!")
            print("âœ… Can run medium models well")
            print("âœ… Gemma 3 4B will work") 
            print("âš¡ Consider 8GB+ for better performance")
            recommendation = "optional_upgrade"
        elif ram_gb >= 2:
            print("âš¡ MINIMAL SETUP")
            print("âš ï¸ Limited to small models")
            print("âœ… Qwen 2B models will work")
            print("ğŸ”„ Upgrade recommended for better AI")
            recommendation = "upgrade_recommended"
        else:
            print("âŒ INSUFFICIENT RAM")
            print("âŒ Cannot run modern AI models effectively")
            print("ğŸ›’ Upgrade required")
            recommendation = "upgrade_required"
        
        print("\nğŸ›’ HARDWARE RECOMMENDATIONS:")
        print("-" * 30)
        
        if recommendation in ["upgrade_recommended", "upgrade_required"]:
            print("ğŸ’» Refurbished Options:")
            print("  â€¢ Intel N110 system: 8GB RAM + SSD")
            print("  â€¢ Intel N95 system: 16GB RAM + SSD")
            print("  â€¢ Expected cost: $300-600")
            print("  â€¢ Benefits: 3-5x better AI performance")
            print()
            print("âš¡ Alternative: RAM upgrade current laptop")
            print("  â€¢ Add 8-16GB RAM if possible")
            print("  â€¢ Much cheaper option")
        else:
            print("âœ… Your current laptop is sufficient!")
            print("ğŸ’¡ Optional: N95 system for dedicated AI server")
            print("ğŸ  Use current laptop + future Jetson setup")
        
        return recommendation

def main():
    setup = KoboldBrainSetup()
    
    print("ğŸ§  KOBOLDCPP + BRAIN INTEGRATION SETUP")
    print("=" * 60)
    print("ğŸ¯ Optimizing your 5 GGUF models for maximum performance")
    print()
    
    # Check if KoboldCpp exists
    if not setup.kobold_exe.exists():
        print(f"âŒ KoboldCpp not found: {setup.kobold_exe}")
        print("ğŸ“¥ Please ensure koboldcpp_rocm_b2.exe is in the current directory")
        return
    
    print("âœ… KoboldCpp found!")
    print()
    
    # Analyze system
    system_specs = setup.check_system_specs()
    
    # Get hardware recommendation
    hardware_rec = setup.assess_hardware_needs(system_specs)
    
    # Get best model recommendation
    best_model, model_message = setup.recommend_best_model(system_specs)
    print(f"\nğŸ¯ RECOMMENDED MODEL: {best_model}")
    print(f"ğŸ’¡ {model_message}")
    
    print("\nğŸš€ SETUP OPTIONS:")
    print("1. ğŸ§ª Test best model with KoboldCpp")
    print("2. ğŸ¯ Start KoboldCpp + configure brain")
    print("3. ğŸ”§ Custom model selection")
    print("4. ğŸ§  Test brain with current setup")
    print("5. ğŸ’¡ Hardware purchase advice")
    print("6. ğŸƒ Exit")
    
    choice = input("\nEnter choice (1-6): ").strip()
    
    if choice == "1" and best_model:
        # Test best model
        process = setup.start_koboldcpp_server(best_model)
        if process:
            time.sleep(5)
            setup.test_kobold_connection()
            
    elif choice == "2" and best_model:
        # Full setup
        process = setup.start_koboldcpp_server(best_model)
        if process:
            time.sleep(5)
            if setup.test_kobold_connection():
                setup.configure_brain_for_kobold()
                print("\nğŸ‰ SETUP COMPLETE!")
                print("âœ… KoboldCpp running with your best model")
                print("âœ… Brain configured to use KoboldCpp")
                print("ğŸš€ Run: python launch_brain_fixed.py")
                
    elif choice == "3":
        # Custom model selection
        print("\nğŸ¯ Available models:")
        for i, (model_id, info) in enumerate(setup.available_models.items(), 1):
            exists = "âœ…" if (setup.models_dir / info["file"]).exists() else "âŒ"
            print(f"{i}. {exists} {model_id} - {info['description']}")
        
        try:
            model_choice = int(input("Select model (1-5): ")) - 1
            model_ids = list(setup.available_models.keys())
            if 0 <= model_choice < len(model_ids):
                selected_model = model_ids[model_choice]
                process = setup.start_koboldcpp_server(selected_model)
        except:
            print("âŒ Invalid selection")
            
    elif choice == "4":
        # Test brain
        print("ğŸ§  Testing brain with current configuration...")
        os.system("python launch_brain_fixed.py --test")
        
    elif choice == "5":
        # Hardware advice
        if hardware_rec == "upgrade_required":
            print("\nğŸ’» HARDWARE PURCHASE RECOMMENDATION:")
            print("ğŸ¯ Intel N95 Mini PC (Recommended)")
            print("  â€¢ 16GB RAM + 512GB SSD")
            print("  â€¢ Price: ~$400-500 refurbished")
            print("  â€¢ Perfect for dedicated AI server")
            print("  â€¢ Can run larger models smoothly")
            print()
            print("âš¡ Intel N110 Alternative")
            print("  â€¢ 8GB RAM + 256GB SSD") 
            print("  â€¢ Price: ~$250-350 refurbished")
            print("  â€¢ Good for medium AI models")
            print()
            print("ğŸ  Recommendation: Get N95 for future-proof AI lab")
        else:
            print("âœ… Your current laptop should work well!")
            print("ğŸ’¡ Consider N95 only for dedicated AI server setup")
    
    elif choice == "6":
        print("ğŸ‘‹ Setup complete!")
    
    else:
        print("âŒ Invalid choice")

if __name__ == "__main__":
    main()